defaults:
  - _self_
  - dataset: TextSummary
  - model: deepseek_7b_4bit
  - machine: gpu
  - paths: save


args:
  _target_: transformers.TrainingArguments
  output_dir: ${paths.save}
  evaluation_strategy: "epoch"
  learning_rate: 3e-4
  per_device_train_batch_size: 1
  gradient_accumulation_steps: 8
  num_train_epochs: 1
  weight_decay: 0.01
  logging_dir: ${paths.logs}
  fp16: True
  report_to: "wandb"                 
  run_name: "1E_deepseek_7b_4bit_lora8"
  logging_steps: 5
  
logger:
  _target_: pytorch_lightning.loggers.WandbLogger
  mode: "online"
  project: SLM_finetuning
  save_dir: ${paths.logs}/wandb
  name: 10E_deepseek_7b_4bit_lora8

  log_model: True

seed: 1999
max_epochs: 10

train: True
test: True
log: True
checkpoints: True

load_checkpoint: False
path_cpt: ${paths.save}/checkpoints/ # Path to the checkpoint to load
